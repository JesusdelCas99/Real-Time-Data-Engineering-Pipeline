version: '3'

services:  

  broker:
    container_name: broker
    image: confluentinc/cp-server:7.4.0
    ports:
      - "9092:9092"
    env_file: ./.env/kafka.env
    networks:
      kafka-network:
        ipv4_address: 172.20.10.3
    healthcheck:
      test: ["CMD", "nc", "-w", "2", "localhost", "9092"]
      interval: 15s
      timeout: 5s
      retries: 3
  
  
  postgres:
    container_name: postgres
    image: postgres:14
    env_file: ./.env/postgres.env
    volumes:
      - ./postgres/data:/var/lib/postgresql/data
    networks:
      kafka-network:
        ipv4_address: 172.20.10.4
  
    
  airflow:
    container_name: airflow
    image: apache/airflow:2.6.0-python3.9
    depends_on: 
      - postgres
      - broker
    ports:
      - "2040:2040"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./requirements.txt:/opt/airflow/requirements.txt
    env_file: ./.env/airflow.env
    command:
      - bash
      - -c
      - |
        pip install -r ./requirements.txt;
        airflow db init;
        airflow users create --username user --firstname user --lastname user --role Admin --email admin@example.com --password admin;
        airflow db upgrade;
        airflow webserver -p 2040 > /dev/null 2>&1 &
        airflow scheduler
    networks:
      kafka-network:
        ipv4_address: 172.20.10.5


  schema-registry:
    image: confluentinc/cp-schema-registry:7.4.0
    container_name: schema-registry
    depends_on:
      broker:
        condition: service_healthy
    ports:
      - "8083:8083"
    env_file: ./.env/schema-registry.env
    networks:
      kafka-network:
        ipv4_address: 172.20.10.6


  control-center:
    container_name: control-center
    image: 'confluentinc/cp-enterprise-control-center:7.4.0'
    env_file: ./.env/control-center.env
    ports:
      - '9021:9021'
    depends_on:
      - broker
      - schema-registry
    networks:
      kafka-network:
        ipv4_address: 172.20.10.7
  

  spark-master:
    image: bitnami/spark:3.5.1
    command: bin/spark-class org.apache.spark.deploy.master.Master
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      kafka-network:
        ipv4_address: 172.20.10.8


  spark-worker:
    image: bitnami/spark:3.5.1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    env_file: ./.env/spark-worker.env
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    networks:
      kafka-network:
        ipv4_address: 172.20.10.9


  cassandra:
    image: cassandra:3.11.11
    container_name: cassandra
    ports:
      - "9042:9042"
    env_file: ./.env/cassandra.env
    volumes:
      - ./cassandra/cassandra-init.cql:/tmp/cassandra-init.cql
      - ./cassandra/init.sh:/tmp/init.sh
      - ./cassandra/data:/var/lib/cassandra
    command:
      - bash
      - -c
      - |
        /docker-entrypoint.sh > /dev/null 2>&1 &
        /tmp/init.sh
    networks:
      kafka-network:
        ipv4_address: 172.20.10.10


  spark-submmit:
    image: bitnami/spark:3.5.1
    depends_on:
      - spark-master
      - spark-worker
      - cassandra
    volumes:
      - ./spark/spark-stream.py:/opt/bitnami/spark-stream.py
      - ./spark/logs:/tmp/spark-events
    command:
      - bash
      - -c
      - |
        spark-submit \
        --master spark://172.20.10.8:7077 \
        --conf "spark.driver.memory=1g" \
        --conf "spark.executor.memory=1g" \
        --conf "spark.eventLog.enabled=true" \
        --conf "spark.eventLog.dir=/tmp/spark-events" \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 \
        --conf "spark.kafka.bootstrap.servers=172.20.10.3:9092" \
        --conf "spark.kafka.group.id=my_spark_cluster" \
        --conf "spark.cassandra.connection.host=172.20.10.10" \
        --conf "spark.cassandra.connection.port=9042" \
        --conf "spark.cassandra.auth.username=cassandra" \
        --conf "spark.cassandra.auth.password=cassandra" \
        --conf "spark.cassandra.output.consistency.level=ONE" \
        /opt/bitnami/spark-stream.py
    networks:
      kafka-network:
        ipv4_address: 172.20.10.11


networks:
  kafka-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.10.0/24